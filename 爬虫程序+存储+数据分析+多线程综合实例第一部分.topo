<?xml version="1.0" encoding="UNICODE" ?>
<topo version="1.2.00.510">
    <devices />
    <lines />
    <shapes />
    <txttips>
        <txttip left="170" top="-283" right="2268" bottom="2323" content="说明：&#x0D;&#x0A;爬虫程序+储存+数据分析+多线程(第一部分)&#x0D;&#x0A;#_*_coding:gb2312utf-8_*_&#x0D;&#x0A;#使用多线程爬虫获取网站中的我们所需的数据&#x0D;&#x0A;from bs4 import BeautifulSoup&#x0D;&#x0A;import numpy as np&#x0D;&#x0A;import pandas as pd&#x0D;&#x0A;import requests&#x0D;&#x0A;import time&#x0D;&#x0A;import threading&#x0D;&#x0A;from sqlalchemy import create_engine&#x0D;&#x0A;def get_url(num,list_urlx):&#x0D;&#x0A; url=&apos;http://cd.newhouse.fang.com/house/s/b9&apos;+str(num)+&apos;/?ctm=1.cd.xf_search.page.1&apos;&#x0D;&#x0A; headers={&apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36&apos;}&#x0D;&#x0A; try:&#x0D;&#x0A;     res=requests.get(url,headers=headers)#当要添加header,防止被过滤&#x0D;&#x0A;     res.encoding=&apos;gb2312&apos;#requests.get的编码使用gb2312&#x0D;&#x0A;     if res.status_code==200:#当res.status_code==200为正常状态&#x0D;&#x0A;       soup=BeautifulSoup(res.text,&apos;html.parser&apos;)#把res.text获取的代码交给BeautifulSoup的html.parser解析器处理&#x0D;&#x0A;       list1=soup.select(&apos;.nlcd_name a&apos;)&#x0D;&#x0A;       for i in range(0,len(list1)):&#x0D;&#x0A;          if soup.select(&apos;.nlcd_name a&apos;)[i][&apos;href&apos;] not in list_urlx:#去重&#x0D;&#x0A;            list_urlx.append(soup.select(&apos;.nlcd_name a&apos;)[i][&apos;href&apos;])#使用循环遍历select函数提取nlcd_name类中的每一个超链接标签中的地址（就是href中的值）,.nlcd_name表示nlcd_name这个类，.nlcd_name a表示nlcd_name这个类中的&lt;a&gt;&lt;/a&gt;标签&#x0D;&#x0A; except Exception as e:&#x0D;&#x0A;       raise e&#x0D;&#x0A;     &#x0D;&#x0A;&#x0D;&#x0A;def get_urlx(list_url,list_url1,list_url2):&#x0D;&#x0A; headers={&apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36&apos;}&#x0D;&#x0A; try:&#x0D;&#x0A;        res=requests.get(&apos;http:&apos;+list_url,headers=headers)&#x0D;&#x0A;        res.encoding=&apos;gb2312&apos;&#x0D;&#x0A;        if res.status_code==200:&#x0D;&#x0A;          soup=BeautifulSoup(res.text,&apos;html.parser&apos;)&#x0D;&#x0A;          list_url1.append(soup.select(&apos;.nav a&apos;)[1][&apos;href&apos;])#使用select函数提取nav类中的第二个超链接标签中的地址（就是href中的值）,.nav表示nav这个类，.nav a表示nav这个类中的&lt;a&gt;&lt;/a&gt;标签&#x0D;&#x0A;          list_url2.append(soup.select(&apos;.nav a&apos;)[3][&apos;href&apos;])#使用select函数提取nav类中的第四个超链接标签中的地址（就是href中的值）,.nav表示nav这个类，.nav a表示nav这个类中的&lt;a&gt;&lt;/a&gt;标签&#x0D;&#x0A; except Exception as e:&#x0D;&#x0A;     raise e&#x0D;&#x0A;&#x0D;&#x0A;     &#x0D;&#x0A;def get_info(list_url1,list_url2,house_list):&#x0D;&#x0A; headers={&apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36&apos;}&#x0D;&#x0A; try:&#x0D;&#x0A;          house={}&#x0D;&#x0A;          area=&apos;&apos;&#x0D;&#x0A;          res=requests.get(&apos;http:&apos;+list_url1,headers=headers)&#x0D;&#x0A;          res.encoding=&apos;gb2312&apos;&#x0D;&#x0A;          res1=requests.get(&apos;http:&apos;+list_url2,headers=headers)&#x0D;&#x0A;          res1.encoding=&apos;gb2312&apos;&#x0D;&#x0A;          if res.status_code==200:&#x0D;&#x0A;             soup=BeautifulSoup(res.text,&apos;html.parser&apos;)&#x0D;&#x0A;             house[&apos;名称&apos;]=soup.select(&apos;.ts_linear&apos;)[0].get_text(strip=&apos;\n,\t&apos;)#使用select函数提取ts_linear类中的第一个标签的内容，并去除&apos;\n,\t&apos;，get_text函数是取出标签中内容并且转为字符串&#x0D;&#x0A;             house[&apos;单价&apos;]=soup.select(&apos;.main_1200 em&apos;)[0].get_text(strip=&apos;\n,\t&apos;)&#x0D;&#x0A;             decorate=soup.select(&apos;.list-right&apos;)[3].get_text(strip=&apos;\n,\t&apos;)  &#x0D;&#x0A;             if len(decorate)&gt;6 and decorate[0:2]!=&apos;毛坯&apos; and decorate[0:3]!=&apos;非毛坯&apos;:&#x0D;&#x0A;                    house[&apos;装修&apos;]=&apos;暂无&apos;&#x0D;&#x0A;             else:&#x0D;&#x0A;                    house[&apos;装修&apos;]=soup.select(&apos;.list-right&apos;)[3].get_text(strip=&apos;\n,\t&apos;)&#x0D;&#x0A;             soup1=BeautifulSoup(res1.text,&apos;html.parser&apos;)&#x0D;&#x0A;             if soup1.select(&apos;.tiaojian span&apos;):&#x0D;&#x0A;                house[&apos;户型&apos;]=soup1.select(&apos;.tiaojian span&apos;)[0].get_text(strip=&apos;\n,\t&apos;)&#x0D;&#x0A;                area=soup1.select(&apos;.tiaojian span&apos;)[1].get_text(strip=&apos;\n,\t&apos;)[0:5]#使用select函数提取tiaojian类中的span标签的第一个标签的内容的前5个字符并去除&apos;\n,\t&apos;，&#x0D;&#x0A;                house[&apos;面积&apos;]=area&#x0D;&#x0A;             #计算出总价&#x0D;&#x0A;             if area:&#x0D;&#x0A;                 if     len(house[&apos;单价&apos;]) == 13 :&#x0D;&#x0A;                    house[&apos;总价&apos;]=int(float(house[&apos;单价&apos;][3:8])* float(area))&#x0D;&#x0A;                 elif  len(house[&apos;单价&apos;]) == 12 and house[&apos;单价&apos;][0:3]== &apos;均价约&apos; :&#x0D;&#x0A;                    house[&apos;总价&apos;]=int(float(house[&apos;单价&apos;][3:7])* float(area))&#x0D;&#x0A;                 elif  len(house[&apos;单价&apos;]) == 12 and house[&apos;单价&apos;][2]!= &apos;约&apos; and house[&apos;单价&apos;][0]!= &apos;约&apos;:&#x0D;&#x0A;                    house[&apos;总价&apos;]=int(float(house[&apos;单价&apos;][2:6])* float(area))&#x0D;&#x0A;                 elif  len(house[&apos;单价&apos;]) == 12 and house[&apos;单价&apos;][0]== &apos;约&apos; :&#x0D;&#x0A;                    house[&apos;总价&apos;]=int(float(house[&apos;单价&apos;][1:6])* float(area))&#x0D;&#x0A;                 elif  len(house[&apos;单价&apos;]) == 11 and house[&apos;单价&apos;][0]== &apos;约&apos;: &#x0D;&#x0A;                    house[&apos;总价&apos;]=int(float(house[&apos;单价&apos;][1:5])* float(area))&#x0D;&#x0A;             else:&#x0D;&#x0A;                house[&apos;总价&apos;]=&apos;暂无&apos;&#x0D;&#x0A;          house_list.append(house)&#x0D;&#x0A; except Exception as e:&#x0D;&#x0A;     raise e&#x0D;&#x0A;        &#x0D;&#x0A;if __name__==&apos;__main__&apos;:  &#x0D;&#x0A; #使用多线程运行函数get_url&#x0D;&#x0A; list_urlx=[]&#x0D;&#x0A; threads1=[]&#x0D;&#x0A; for i in range(1,41):&#x0D;&#x0A;   t=threading.Thread(target=get_url,args=(i,list_urlx))#把函数装入线程中，target为函数名，args为传入的参数&#x0D;&#x0A;   threads1.append(t)&#x0D;&#x0A; for i in range(0,40):&#x0D;&#x0A;    threads1[i].start()#使用遍历启动每一个线程&#x0D;&#x0A; for i in range(0,40):&#x0D;&#x0A;    threads1[i].join()#使每一个线程都结束后，才执行后面的代码&#x0D;&#x0A; print(list_urlx)&#x0D;&#x0A; print(len(list_urlx)) &#x0D;&#x0A;#使用多线程运行函数get_urlx&#x0D;&#x0A; list_url1=[]&#x0D;&#x0A; list_url2=[] &#x0D;&#x0A; threads2=[]&#x0D;&#x0A; for i in range(0,len(list_urlx)):&#x0D;&#x0A;    t=threading.Thread(target=get_urlx,args=(list_urlx[i],list_url1,list_url2))&#x0D;&#x0A;    threads2.append(t)&#x0D;&#x0A; for i in range(0,len(list_urlx)):&#x0D;&#x0A;    time.sleep(1)&#x0D;&#x0A;    threads2[i].start()&#x0D;&#x0A; for i in range(0,len(list_urlx)):&#x0D;&#x0A;    threads2[i].join()&#x0D;&#x0A; #使用多线程运行函数get_infoget_urlx&#x0D;&#x0A; threads=[]&#x0D;&#x0A; house_list=[]&#x0D;&#x0A; for i in range(0,len(list_url1)):&#x0D;&#x0A;   t=threading.Thread(target=get_info,args=(list_url1[i],list_url2[i],house_list))&#x0D;&#x0A;   threads.append(t)&#x0D;&#x0A; for i in range(0,len(list_url1)):&#x0D;&#x0A;     time.sleep(1)&#x0D;&#x0A;     threads[i].start()&#x0D;&#x0A;     time.sleep(1)&#x0D;&#x0A; for i in range(0,len(list_url1)):&#x0D;&#x0A;      threads[i].join()   &#x0D;&#x0A;#-----------------------------------------------使用多线程爬虫获取网站中的我们所需的数据&#x0D;&#x0A;#数据爬取的数据存入文件或者是数据库&#x0D;&#x0A;from sqlalchemy import create_engine #如果没有sqlalchemy模块要使用pip安装(pip install flask-SQLAlchemy),python3.5以后都有pip，但是要把pip文件夹加入环境变量&#x0D;&#x0A;import pymysql        #如果没有pymysql模块要使用pip安装，anaconda默认命令行有pip&#x0D;&#x0A;import pandas as pd&#x0D;&#x0A;columns=(&apos;名称&apos;,&apos;单价&apos;,&apos;装修&apos;,&apos;户型&apos;,&apos;面积&apos;,&apos;总价&apos;)&#x0D;&#x0A;df=pd.DataFrame(house_list,columns=columns)#使用爬取的数据列表创建DataFrame&#x0D;&#x0A;# root是mysql数据库用户名，RSlf2017是mysql数据库密码,192.168.196.128:3306是mysql数据库地址和端口号，house是数据库名称，charset=utf8设置连接的字符集为utf8&#x0D;&#x0A;conn=create_engine(&apos;mysql+pymysql://root:RSlf2017@192.168.196.128:3306/house?charset=utf8&apos;)#创建跟mysql数据库的连接，字段的字符集要设置成utf8 ，否则要报错，因为默认不支持中文&#x0D;&#x0A;#修改数据库enterprises的字符集&#x0D;&#x0A;#alter database enterprises character set utf8&#x0D;&#x0A;#修改数据表employees的字符集：&#x0D;&#x0A;#alter table employees character set utf8&#x0D;&#x0A;#修改字段的字符集&#x0D;&#x0A;#alter table employees change name name char(10) character set utf-8;&#x0D;&#x0A;#df是要存入数据库的ataFrame，&apos;house_price&apos;是表名称，conn是连接，schema=&apos;house&apos;是数据库名称，if_exists=&apos;append&apos;表示数据存在就再次添加&#x0D;&#x0A;pd.io.sql.to_sql(df,&apos;house_price&apos;,conn,schema=&apos;house&apos;,if_exists=&apos;append&apos;)#把构造的DataFrame存入mysql数据库中(使用orm)&#x0D;&#x0A;df.to_csv(&apos;1.txt&apos;)#把构造的DataFrame写入csv文件1.txt中&#x0D;&#x0A;df=pd.DataFrame() #把内存里面的数据制空&#x0D;&#x0A;df2=pd.read_csv(&apos;1.txt&apos;)#从csv文件1.txt中读取全面数据赋值给df2变量&#x0D;&#x0A;#-----------------------------------------------数据爬取的数据存入文件或者是数据库" fontname="Consolas" fontstyle="0" editsize="120" txtcolor="-16777216" txtbkcolor="-7278960" charset="1" />
    </txttips>
</topo>
